{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11297ba6",
   "metadata": {},
   "source": [
    "# Módulo 24 - Tarefa 02 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a38116",
   "metadata": {},
   "source": [
    "- Cite 5 diferenças entre o AdaBoost e o GBM.\n",
    "- Acesse o link Scikit-learn - GBM, leia a explicação (traduza se for preciso) e crie um jupyter notebook contendo o exemplo de classificação e de regressão do GBM.\n",
    "- Cite 5 Hyperparametros importantes no GBM.\n",
    "- Acessando o artigo do Jerome Friedman Stochastic e pensando no nome dado ao Stochastic GBM, qual é a maior diferença entre os dois algoritmos?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1ece62",
   "metadata": {},
   "source": [
    "### 1. Árvores de Decisão vs Stumps de Decisão\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcf2bab",
   "metadata": {},
   "source": [
    "O Gradient Boosting Machine (GBM) é um algoritmo de aprendizado de máquina que utiliza um conjunto de árvores de decisão, conhecidas como \"árvores fracas\", onde cada árvore é ajustada para corrigir os erros cometidos pelas árvores anteriores. Essas árvores fracas geralmente têm profundidades variadas, o que permite ao GBM aprender relacionamentos complexos nos dados. (Se destaca na modelagem de relacionamentos complexos). \n",
    "\n",
    "Por outro lado, o algoritmo AdaBoost (Adaptive Boosting) também se baseia no uso de árvores de decisão, mas com uma estrutura mais simples chamada \"Stump\" ou \"Stump de decisão\". Esses Stumps consistem em apenas um nó de decisão e duas folhas, tornando-os árvores muito rasas. O AdaBoost funciona selecionando repetidamente os recursos mais relevantes e criando esses Stumps, dando maior peso às instâncias classificadas incorretamente, de modo que os próximos Stumps priorizem a correção desses erros, resultando em um modelo robusto. (Se destaca em modelos que necessitam de precisão e preferencialmente mais simples). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a679f16",
   "metadata": {},
   "source": [
    "**Pesos**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48d9cae",
   "metadata": {},
   "source": [
    "No GBM, cada árvore contribui com um valor preditivo adicional para a decisão final, e essas contribuições são ponderadas igualmente. Essencialmente, todas as árvores têm o mesmo peso na decisão final e são multiplicadas por uma constante chamada taxa de aprendizado (eta). O GBM busca minimizar o erro residual a cada interação, ajustando as árvores subsequentes para corrigir as deficiências dos modelos anteriores.\n",
    "\n",
    "Já o algoritmo AdaBoost adota uma abordagem diferente ao atribuir pesos às árvores (Stumps). Em vez de dar peso igual a todas as árvores, o AdaBoost atribui pesos diferentes a cada Stump com base em sua capacidade de classificação. Após a primeira interação, o AdaBoost aumenta o peso das instâncias classificadas incorretamente e diminui o peso das classificadas corretamente. Isso faz com que os Stumps seguintes concentrem-se mais nas instâncias que foram previamente classificadas de forma errada, melhorando a precisão geral do modelo.\n",
    "\n",
    "Em suma o GBM foca na correção do erro residual em cada passo, enquanto o AdaBoost prioriza as instâncias mais difíceis de classificar, permitindo que os Stumps mais relevantes tenham maior influência na decisão final do modelo. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8660ac35",
   "metadata": {},
   "source": [
    "**Iniciando**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad264976",
   "metadata": {},
   "source": [
    "No GBM, o primeiro passo envolve calcular a média da variável resposta (ou valor alvo) de todo o conjunto de treinamento. Em seguida, o algoritmo inicia com uma árvore de decisão simples (também conhecida como \"árvore inicial\" ou \"baseline tree\"), que prevê a média da variável resposta para todas as instâncias do conjunto de dados.\n",
    "\n",
    "No AdaBoost, o primeiro passo é atribuir pesos iguais a todas as instâncias do conjunto de treinamento. Em seguida, é construído o primeiro Stump de decisão (árvore de decisão com apenas um nó e duas folhas) com base nos pesos das instâncias. O objetivo é encontrar o Stump que melhor classifica as instâncias, focando mais nas que possuem pesos maiores (ou seja, aquelas que foram classificadas incorretamente na etapa anterior).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bf8fe5",
   "metadata": {},
   "source": [
    "**Resíduo vs Resposta**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd0a6ae",
   "metadata": {},
   "source": [
    "No GBM, cada árvore de decisão é criada com o objetivo de prever o resíduo (diferença entre o valor verdadeiro e o valor previsto) ou ($$ Y - \\hat{Y} $$)do modelo atual em relação aos dados de treinamento. Inicialmente, o GBM começa com uma previsão simples, como a média da variável resposta, e, em seguida, constrói árvores adicionais para capturar os padrões residuais não explicados pelo modelo atual. Cada nova árvore é ajustada para minimizar os resíduos do modelo anterior, permitindo que o GBM se concentre nas áreas onde o modelo ainda precisa ser aprimorado.\n",
    "\n",
    "No algoritmo AdaBoost, cada Stump fornece uma resposta ponderada para cada instância do conjunto de treinamento. Essas respostas são combinadas, considerando os pesos atribuídos a cada Stump, por meio de uma média ou votação ponderada, para chegar à resposta final do modelo. A ideia é que, ao atribuir maior peso às instâncias classificadas incorretamente pelos Stumps anteriores, o algoritmo dê mais importância à correção dessas instâncias nas próximas iterações.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94f468f",
   "metadata": {},
   "source": [
    "**Base para a Árvore**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3503c79",
   "metadata": {},
   "source": [
    "No GBM cada árvore de decisão é construída com base nos resíduos do modelo anterior. Os resíduos são as diferenças entre os valores verdadeiros (observados) e os valores previstos pelo modelo até o momento $$ Y - \\hat{Y} $$. Cada nova árvore é ajustada para capturar e corrigir os padrões residuais que ainda não foram adequadamente explicados pelo modelo atual. Dessa forma, o GBM foca em melhorar a previsão progressivamente, tornando-se mais preciso à medida que as árvores são adicionadas.\n",
    "\n",
    "No algoritmo AdaBoost, a base de dados utilizada para construir cada Stump varia dinamicamente em cada ineração. Essa variação ocorre em função dos resultados obtidos pelo Stump anterior. Em outras palavras, o algoritmo atribui pesos diferentes a cada instância da base de dados, priorizando aquelas que foram classificadas incorretamente pelo Stump anterior. Dessa forma, o próximo Stump é construído levando em consideração as instâncias mais desafiadoras e com maior peso, buscando corrigir os erros cometidos nas iterações anteriores.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a12c7e7",
   "metadata": {},
   "source": [
    "# 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "334d61d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import make_friedman1\n",
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d698e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_hastie_10_2(random_state=0)\n",
    "X_train, X_test = X[:2000], X[2000:]\n",
    "y_train, y_test = y[:2000], y[2000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a34d21c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52666a0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.913"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "815d353a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_friedman1(n_samples=1200, random_state=0, noise=1.0)\n",
    "X_train, X_test = X[:200], X[200:]\n",
    "y_train, y_test = y[:200], y[200:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93b684be",
   "metadata": {},
   "outputs": [],
   "source": [
    "est = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, \n",
    "                                max_depth=1, random_state=0, loss='squared_error').fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b12fd035",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.009154859960321"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test, est.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9377b1",
   "metadata": {},
   "source": [
    "# 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6710e9a",
   "metadata": {},
   "source": [
    "**n_estimators**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0672d161",
   "metadata": {},
   "source": [
    "O parâmetro \"n_estimators\" representa o número de etapas ou iterações que o algoritmo de Boosting realizará. No caso do Gradient Boosting, quanto maior o valor de \"n_estimators\", melhor a performance geralmente se torna. Isso acontece porque o algoritmo é projetado para reduzir o viés do modelo e melhorar sua capacidade preditiva com cada nova iteração.\n",
    "\n",
    "Uma característica interessante do Gradient Boosting é que ele é geralmente robusto a overfitting, mesmo quando um número maior de estimadores é usado. Isso ocorre porque, em cada etapa, o modelo tenta corrigir os erros do modelo anterior, permitindo que ele se concentre em padrões residuais, e não em dados já bem ajustados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2d85e0",
   "metadata": {},
   "source": [
    "**learning_rate**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2283efc4",
   "metadata": {},
   "source": [
    "O parâmetro \"learning_rate\" (taxa de aprendizado) é um fator crítico no algoritmo de Gradient Boosting, pois influencia a contribuição de cada árvore para o modelo final. O learning_rate reduz a importância das árvores no conjunto, controlando o quanto cada árvore corrige os erros do modelo nas iterações subsequentes.\n",
    "\n",
    "Ao diminuir o valor do learning_rate, cada árvore tem uma contribuição mais suave no processo de correção dos erros. Isso resulta em um ajuste mais lento do modelo, pois cada árvore tem menos impacto individual. No entanto, um learning_rate menor pode aumentar a capacidade de generalização do modelo, tornando-o menos suscetível ao overfitting e mais estável em diferentes conjuntos de dados.\n",
    "\n",
    "Por outro lado, aumentar o learning_rate dá mais importância a cada árvore, tornando o ajuste do modelo mais rápido. Isso pode levar a um ajuste preciso dos dados de treinamento, mas também pode aumentar o risco de overfitting, especialmente se o número de estimadores (n_estimators) for grande. Portanto, ao aumentar o learning_rate, é essencial monitorar de perto o desempenho do modelo em um conjunto de validação para evitar overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2407e8f",
   "metadata": {},
   "source": [
    "**min_samples_leaf**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddc1573",
   "metadata": {},
   "source": [
    "O parâmetro \"min_samples_leaf\" representa o número mínimo de amostras que devem estar presentes em um nó folha ao realizar a construção de uma árvore de decisão. É o critério que determina se um ponto de divisão pode ocorrer em qualquer profundidade da árvore. Para que um ponto de divisão seja considerado, ele deve deixar pelo menos \"min_samples_leaf\" em amostras de treinamento em cada um dos ramos esquerdo e direito.\n",
    "\n",
    "Definir um valor maior para \"min_samples_leaf\" tem o efeito de suavizar o modelo, especialmente em problemas de regressão. Ao estipular um número mínimo maior de amostras em um nó folha, a árvore será mais restrita em seu crescimento, evitando ajustes excessivamente detalhados aos dados de treinamento. Isso pode ser benéfico para reduzir o overfitting, pois a árvore terá menos chances de capturar ruídos e padrões irrelevantes do conjunto de treinamento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f7e6a5",
   "metadata": {},
   "source": [
    "**max_depth**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa6b8fd",
   "metadata": {},
   "source": [
    "O parâmetro \"max_depth\" representa a profundidade máxima permitida para cada estimador (árvore de decisão) individual em um algoritmo de aprendizado de máquina, especialmente em regressão. Essa profundidade máxima define o número máximo de nós que a árvore pode ter.\n",
    "\n",
    "Ao definir um valor para \"max_depth\", nós limitamos a expansão da árvore, impedindo que ela se torne muito profunda e complexa. Árvores com maior profundidade podem aprender padrões detalhados dos dados de treinamento, mas também estão mais propensas a superajustar (overfitting) aos dados. Isso significa que a árvore pode memorizar o ruído presente no conjunto de treinamento, tornando-se menos capaz de generalizar para dados não vistos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed62ad28",
   "metadata": {},
   "source": [
    "**ccp_alpha**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea53bd7",
   "metadata": {},
   "source": [
    "O parâmetro \"ccp_alpha\" é utilizado para o processo de Minimal Cost-Complexity Pruning (poda de complexidade mínima), que é uma técnica usada para melhorar a generalização de árvores de decisão. Essa técnica visa evitar overfitting, simplificando a árvore por meio da remoção de subárvores que não contribuem significativamente para a melhoria da precisão do modelo.\n",
    "\n",
    "Quando se realiza a poda de complexidade mínima, é calculada uma métrica chamada complexidade de custo para cada subárvore da árvore de decisão. Essa complexidade de custo é uma medida da qualidade do ajuste da subárvore em relação aos dados de treinamento e da sua complexidade (tamanho). Em seguida, é selecionada a subárvore com a maior complexidade de custo que seja menor do que o valor especificado em \"ccp_alpha\".\n",
    "\n",
    "Ao ajustar o valor de \"ccp_alpha\", você controla o grau de poda que será aplicado à árvore. Valores maiores de \"ccp_alpha\" resultam em uma poda mais agressiva, tornando a árvore mais simples e evitando ajustes excessivos aos dados de treinamento. Por outro lado, valores menores de \"ccp_alpha\" preservam mais nós da árvore, tornando-a mais complexa e detalhada.\n",
    "\n",
    "A escolha apropriada para \"ccp_alpha\" é mais um aspecto de ajuste de hiperparâmetros que depende das características do conjunto de dados e do compromisso desejado entre precisão e simplicidade do modelo. É comum utilizar técnicas de validação cruzada para encontrar o valor ideal de \"ccp_alpha\" que ofereça o melhor equilíbrio entre complexidade do modelo e sua capacidade de generalização.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1d6a7f",
   "metadata": {},
   "source": [
    "# 4.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb51ac6",
   "metadata": {},
   "source": [
    "Uma das principais diferenças do Stochastic Gradient Boosting Machine (Stochastic GBM) em relação ao GBM tradicional é a incorporação da aleatoriedade como parte integral do processo. Em cada interação do Stochastic GBM, uma sub-amostra aleatória (sem reposição) é extraída da base de treinamento completa. Essa sub-amostra é usada para treinar a árvore de decisão naquela iteração específica.\n",
    "\n",
    "Essa etapa de amostragem aleatória é o que torna o Stochastic GBM \"estocástico\", uma vez que a aleatoriedade está presente no processo de construção das árvores. Essa abordagem introduz um elemento de variação a cada interação, tornando o Stochastic GBM mais robusto e menos suscetível a overfitting em comparação com o GBM tradicional.\n",
    "\n",
    "No Gradient Boosting Machine (GBM) tradicional, todas as árvores são construídas utilizando o conjunto completo de treinamento em cada interação. Isso pode tornar o modelo mais sensível aos ruídos e outliers presentes nos dados de treinamento, o que pode levar a um ajuste excessivo (overfitting) se o número de estimadores for muito grande.\n",
    "\n",
    "Ao incorporar a amostragem aleatória no processo de construção das árvores, o Stochastic GBM consegue generalizar melhor para dados não vistos e reduzir a dependência de determinadas instâncias ou características. Essa técnica é particularmente útil quando se lida com conjuntos de dados de grande escala, onde a variação introduzida pelo Stochastic GBM pode ajudar a obter modelos mais eficientes e precisos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48873c7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
